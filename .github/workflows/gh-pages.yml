name: Deploy Documentation

on:
  push:
    branches:
      - main
    tags:
      - '*'
  release:
    types:
      - published

permissions:
  id-token: write
  contents: write
  pages: write
  actions: read

jobs:
  test-notebooks:
    runs-on: ubuntu-latest
    name: "Test Notebooks"
    strategy:
      fail-fast: false
      matrix:
        group: [1, 2, 3, 4]

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Required for setuptools_scm version calculation

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "**/pyproject.toml"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install system dependencies
        run: sudo apt-get update && sudo apt-get install -y libopenmpi-dev ffmpeg

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Execute notebooks in parallel
        id: execute
        shell: bash
        env:
          GROUP: ${{ matrix.group }}
          TOTAL_GROUPS: 4
          SMOKE_TEST: "True"
        run: |
          set +e  # Don't exit on first error
          NOTEBOOKS=$(find . -type f -name "*.ipynb" -not -path '*/.*' -not -path '*Xparallel*')
          NOTEBOOK_ARRAY=($NOTEBOOKS)
          # Create temp directory for tracking
          TMPDIR=$(mktemp -d)
          trap "rm -rf $TMPDIR" EXIT
          SCRIPT_START=$(date +%s)
          # Function to execute a notebook and track results
          execute_notebook() {
              local file=$1
              local tmpdir=$2
              # Add random jitter (0-2 seconds) to desynchronize parallel starts
              sleep $(awk -v seed=$RANDOM 'BEGIN{srand(seed); printf "%.2f", rand() * 2}')

              START_TIME=$(date +%s)

              # Retry logic for handling port collisions
              MAX_RETRIES=3
              RETRY_COUNT=0
              SUCCESS=false

              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                  if uv run jupyter nbconvert --to notebook --execute "$file" --inplace \
                        --ExecutePreprocessor.timeout=600 2>&1 | tee /tmp/nbconvert_output_$$.log; then
                      SUCCESS=true
                      break
                  else
                      # Check if error is due to port collision
                      if grep -q "Address already in use\|ZMQError\|Kernel died before replying" /tmp/nbconvert_output_$$.log; then
                          RETRY_COUNT=$((RETRY_COUNT + 1))
                          if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                              WAIT_TIME=$((2 ** RETRY_COUNT))  # Exponential backoff: 2, 4, 8 seconds
                              echo "âš ï¸  Port collision detected for $file, retrying in ${WAIT_TIME}s (attempt $((RETRY_COUNT + 1))/$MAX_RETRIES)..." >&2
                              sleep $WAIT_TIME
                          else
                              echo "âœ— Failed to execute $file after $MAX_RETRIES attempts (port collision)" >&2
                          fi
                      else
                          # Non-port-collision error, don't retry
                          break
                      fi
                  fi
              done

              rm -f /tmp/nbconvert_output_$$.log

              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))

              if [ "$SUCCESS" = true ]; then
                  echo "$DURATION" > "$tmpdir/$(basename $file).status"
                  echo "âœ“ Successfully executed $file (${DURATION}s)"
              else
                  echo "failed:$DURATION" > "$tmpdir/$(basename $file).status"
                  echo "âœ— Failed to execute $file (${DURATION}s)" >&2
              fi
          }
          export -f execute_notebook
          # Create directory for executed notebooks
          mkdir -p executed-notebooks
          # Execute notebooks in this group in parallel (2 at a time)
          for ((i=GROUP-1; i<${#NOTEBOOK_ARRAY[@]}; i+=TOTAL_GROUPS)); do
              execute_notebook "${NOTEBOOK_ARRAY[$i]}" "$TMPDIR" &
          done
          wait
          # Copy executed notebooks to a separate directory for uploading
          for ((i=GROUP-1; i<${#NOTEBOOK_ARRAY[@]}; i+=TOTAL_GROUPS)); do
              file="${NOTEBOOK_ARRAY[$i]}"
              # Preserve directory structure
              target_dir="executed-notebooks/$(dirname "$file")"
              mkdir -p "$target_dir"
              cp "$file" "$target_dir/"
          done
          # Collect results
          TOTAL=0
          SUCCESS=0
          FAILED=0
          TOTAL_TIME=0
          echo "## Group $GROUP Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          for ((i=GROUP-1; i<${#NOTEBOOK_ARRAY[@]}; i+=TOTAL_GROUPS)); do
              file="${NOTEBOOK_ARRAY[$i]}"
              TOTAL=$((TOTAL + 1))
              status_file="$TMPDIR/$(basename $file).status"
              if [ -f "$status_file" ]; then
                  status=$(cat "$status_file")
                  if [[ "$status" == failed:* ]]; then
                      FAILED=$((FAILED + 1))
                      duration=${status#failed:}
                      TOTAL_TIME=$((TOTAL_TIME + duration))
                      echo "- âŒ \`$file\` (${duration}s)" >> $GITHUB_STEP_SUMMARY
                  else
                      SUCCESS=$((SUCCESS + 1))
                      TOTAL_TIME=$((TOTAL_TIME + status))
                      echo "- âœ… \`$file\` (${status}s)" >> $GITHUB_STEP_SUMMARY
                  fi
              fi
          done
          SCRIPT_END=$(date +%s)
          SCRIPT_DURATION=$((SCRIPT_END - SCRIPT_START))
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Summary for Group $GROUP:**" >> $GITHUB_STEP_SUMMARY
          echo "- Total: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- Success: $SUCCESS" >> $GITHUB_STEP_SUMMARY
          echo "- Failed: $FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- Total execution time: ${TOTAL_TIME}s" >> $GITHUB_STEP_SUMMARY
          echo "- Wall clock time: ${SCRIPT_DURATION}s" >> $GITHUB_STEP_SUMMARY
          # Export for next step
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "success=$SUCCESS" >> $GITHUB_OUTPUT
          echo "total=$TOTAL" >> $GITHUB_OUTPUT

          [ $FAILED -gt 0 ] && exit 1 || exit 0

      - name: Save results for summary
        if: always()
        run: |
          mkdir -p results
          echo "${{ steps.execute.outputs.success }},${{ steps.execute.outputs.failed }},${{ steps.execute.outputs.total }}" > results/group-${{ matrix.group }}-results.txt

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebook-results-group-${{ matrix.group }}
          path: results/group-${{ matrix.group }}-results.txt

      - name: Upload executed notebooks
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: executed-notebooks-group-${{ matrix.group }}
          path: executed-notebooks/**/*.ipynb

      - name: Minimize uv cache
        run: uv cache prune --ci

  summary:
    name: "Notebook Execution Summary"
    needs: test-notebooks
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: notebook-results-group-*
          merge-multiple: true

      - name: Generate overall summary
        run: |
          echo "# ðŸ“Š Notebook Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          TOTAL_SUCCESS=0
          TOTAL_FAILED=0
          TOTAL_NOTEBOOKS=0

          echo "## Results by Group" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Group | Total | Success âœ… | Failed âŒ |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|-----------|----------|" >> $GITHUB_STEP_SUMMARY

          for i in 1 2 3 4; do
            if [ -f "group-$i-results.txt" ]; then
              IFS=',' read -r success failed total < "group-$i-results.txt"
              echo "| $i | $total | $success | $failed |" >> $GITHUB_STEP_SUMMARY
              TOTAL_SUCCESS=$((TOTAL_SUCCESS + success))
              TOTAL_FAILED=$((TOTAL_FAILED + failed))
              TOTAL_NOTEBOOKS=$((TOTAL_NOTEBOOKS + total))
            else
              echo "| $i | - | - | âš ï¸ No results |" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“ˆ Overall Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Notebooks**: $TOTAL_NOTEBOOKS" >> $GITHUB_STEP_SUMMARY
          echo "- **âœ… Successful**: $TOTAL_SUCCESS" >> $GITHUB_STEP_SUMMARY
          echo "- **âŒ Failed**: $TOTAL_FAILED" >> $GITHUB_STEP_SUMMARY

          if [ $TOTAL_FAILED -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **Some notebooks failed.** Check individual group summaries above for details." >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸŽ‰ **All notebooks executed successfully!**" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Check individual group job summaries for detailed notebook-by-notebook results.*" >> $GITHUB_STEP_SUMMARY

  build:
    needs: [test-notebooks, summary]
    runs-on: ubuntu-latest
    name: "Build Documentation"

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Required for setuptools_scm version calculation

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "**/pyproject.toml"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --extra doc

      - name: Download executed notebooks
        uses: actions/download-artifact@v4
        with:
          pattern: executed-notebooks-group-*
          path: downloaded-notebooks
          merge-multiple: true

      - name: Replace original notebooks with executed versions
        run: |
          # Copy executed notebooks to replace originals
          # Artifacts preserve the structure from executed-notebooks/ which includes ./docs/examples/
          find downloaded-notebooks -name "*.ipynb" -type f | while read notebook; do
            # Extract path after downloaded-notebooks/ (will be like ./docs/examples/...)
            relative_path=$(echo "$notebook" | sed 's|downloaded-notebooks/||')
            # Clean up leading ./ if present
            relative_path=$(echo "$relative_path" | sed 's|^\./||')
            target_path="$relative_path"
            echo "Replacing $target_path with executed version from $notebook"
            cp "$notebook" "$target_path"
          done

      - name: Build docs
        run: uv run mkdocs build

      - name: Extract examples
        run: |
          zip -r xopt-examples.zip docs/examples/
          mv xopt-examples.zip ./site/assets/

      - name: Upload docs artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

      - name: Minimize uv cache
        run: uv cache prune --ci

  deploy:
    if: ${{ github.repository_owner == 'xopt-org' }}
    needs: build
    runs-on: ubuntu-latest
    name: "Deploy to GitHub Pages"

    # Grant GITHUB_TOKEN the permissions required to make a Pages deployment
    permissions:
      pages: write      # to deploy to Pages
      id-token: write   # to verify the deployment originates from an appropriate source

    # Deploy to the github-pages environment
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
