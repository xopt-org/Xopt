import os.path
import warnings
from copy import deepcopy
from typing import Any, Dict, List, Literal, Optional, Union

import botorch.settings
import pandas as pd
import torch
from botorch import fit_gpytorch_mll
from botorch.models import ModelListGP, SingleTaskGP
from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel
from botorch.models.transforms import Normalize, Standardize
from gpytorch import ExactMarginalLogLikelihood
from gpytorch.constraints import GreaterThan
from gpytorch.kernels import Kernel
from gpytorch.likelihoods import GaussianLikelihood, Likelihood
from gpytorch.priors import GammaPrior, Prior
from pydantic import ConfigDict, Field, field_validator
from pydantic_core.core_schema import ValidationInfo
from torch.nn import Module

from xopt.generators.bayesian.base_model import ModelConstructor
from xopt.generators.bayesian.models.prior_mean import CustomMean
from xopt.generators.bayesian.utils import get_training_data
from xopt.pydantic import decode_torch_module

DECODERS = {"torch.float32": torch.float32, "torch.float64": torch.float64}
MIN_INFERRED_NOISE_LEVEL = 1e-4


class StandardModelConstructor(ModelConstructor):
    """
    A class for constructing independent models for each objective and constraint.

    Attributes
    ----------
    name : str
        The name of the model (frozen).

    use_low_noise_prior : bool
        Specify if the model should assume a low noise environment.

    covar_modules : Dict[str, Kernel]
        Covariance modules for GP models.

    mean_modules : Dict[str, Module]
        Prior mean modules for GP models.

    trainable_mean_keys : List[str]
        List of prior mean modules that can be trained.


    Methods
    -------
    get_likelihood
        Get the likelihood for the model, considering the low noise prior.

    build_model(input_names, outcome_names, data, input_bounds, dtype, device)
        Construct independent models for each objective and constraint.

    build_mean_module(name, input_transform, outcome_transform)
        Build the mean module for the output specified by name.

    """

    name: str = Field("standard", frozen=True)
    use_low_noise_prior: bool = Field(
        False, description="specify if model should assume a low noise environment"
    )
    covar_modules: Dict[str, Kernel] = Field(
        {}, description="covariance modules for GP models"
    )
    mean_modules: Dict[str, Module] = Field(
        {}, description="prior mean modules for GP models"
    )
    trainable_mean_keys: List[str] = Field(
        [], description="list of prior mean modules that can be trained"
    )
    transform_inputs: Union[Dict[str, bool], bool] = Field(
        True,
        description="specify if inputs should be transformed inside the gp "
        "model, can optionally specify a dict of specifications",
    )
    custom_noise_prior: Optional[Prior] = Field(
        None,
        description="specify custom noise prior for the GP likelihood, "
        "overwrites value specified by use_low_noise_prior",
    )
    use_cached_hyperparameters: Optional[bool] = Field(
        False,
        description="flag to specify if cached hyperparameters should be used in "
        "model creation instead of training",
    )
    _hyperparameter_store: Optional[Dict] = None
    method: Literal["lbfgs", "adam"] = Field(
        "lbfgs", description="training method to use"
    )
    train_model: bool = Field(
        True,
        description="specify if the model should be trained",
    )
    train_kwargs: Dict[str, Any] | None = Field(
        default_factory=lambda: {
            "maxiter": 1000,
        },
        description="kwargs for training the model - passed to botorch.fit_gpytorch_mll_scipy",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True, validate_assignment=True)

    def __init__(self, **kwargs: Any):
        super().__init__(**kwargs)
    
    @field_validator("train_kwargs")
    def validate_train_kwargs(cls, train_kwargs):
        if train_kwargs is None:
            return train_kwargs
        allowed_keys = [
            "optimizer_kwargs",
            "pick_best_of_all_attempts",
            "max_attempts",
            "optimizer",
            "warning_handler",
        ]
        allowed_subkeys = {"optimizer_kwargs": ["timeout_sec", "options"]}
        if not isinstance(train_kwargs, dict):
            raise ValueError(f"train_kwargs must be a dict, not {type(train_kwargs)}")
        if set(train_kwargs.keys()) - set(allowed_keys):
            raise ValueError(f"train_kwargs can only contain the keys {allowed_keys}")
        for k, v in train_kwargs.items():
            if isinstance(v, dict):
                allowed = allowed_subkeys.get(k, [])
                if set(v.keys()) - set(allowed):
                    raise ValueError(
                        f"train_kwargs['{k}'] can only contain the keys {allowed}"
                    )
        return train_kwargs

    @field_validator("covar_modules", "mean_modules", mode="before")
    def validate_torch_modules(cls, value: Any):
        if not isinstance(value, dict):
            raise ValueError("must be dict")
        else:
            value = cast(dict[str, Any], value)
            for key, val in value.items():
                if isinstance(val, str):
                    if val.startswith("base64:"):
                        value[key] = decode_torch_module(val)
                    elif os.path.exists(val):
                        value[key] = torch.load(val, weights_only=False)

        return value

    @field_validator("trainable_mean_keys")
    def validate_trainable_mean_keys(cls, value: Any, info: ValidationInfo):
        for name in value:
            assert name in info.data["mean_modules"]
        return value

    def get_likelihood(
        self,
        dtype: torch.dtype = torch.double,
        device: Union[torch.device, str] = "cpu",
    ) -> Likelihood:
        """
        Get the likelihood for the model, considering the low noise prior and or a
        custom noise prior.

        Returns
        -------
        Likelihood
            The likelihood for the model.

        """
        tkwargs = {"dtype": dtype, "device": device}
        if self.custom_noise_prior is not None:
            likelihood = GaussianLikelihood(
                noise_prior=self.custom_noise_prior,
            )
        elif self.use_low_noise_prior:
            likelihood = GaussianLikelihood(
                noise_prior=GammaPrior(1.0, 100.0),
            )
        else:
            noise_prior = GammaPrior(1.1, 0.05)
            noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate
            likelihood = GaussianLikelihood(
                noise_prior=noise_prior,
                noise_constraint=GreaterThan(
                    MIN_INFERRED_NOISE_LEVEL,
                    transform=None,
                    initial_value=noise_prior_mode,
                ),
            )
        likelihood = likelihood.to(**tkwargs)
        return likelihood

    def build_model(
        self,
        input_names: List[str],
        outcome_names: List[str],
        data: pd.DataFrame,
        input_bounds: Dict[str, List] = None,
        dtype: torch.dtype = torch.double,
        device: Union[torch.device, str] = "cpu",
    ) -> ModelListGP:
        """
        Construct independent models for each objective and constraint.

        Parameters
        ----------
        input_names : List[str]
            Names of input variables.
        outcome_names : List[str]
            Names of outcome variables.
        data : pd.DataFrame
            Data used for training the model.
        input_bounds : Dict[str, List], optional
            Bounds for input variables.
        dtype : torch.dtype, optional
            Data type for the model (default is torch.double).
        device : Union[torch.device, str], optional
            Device on which to perform computations (default is "cpu").

        Returns
        -------
        ModelListGP
            A list of trained botorch models.

        """
        # build model
        tkwargs = {"dtype": dtype, "device": device}
        models = []

        # validate if model caching can be used if requested
        if self.use_cached_hyperparameters:
            if self._hyperparameter_store is None:
                raise RuntimeWarning(
                    "cannot use cached hyperparameters, hyperparameter store empty, "
                    "training GP model hyperparameters instead"
                )

        covar_modules = deepcopy(self.covar_modules)
        mean_modules = deepcopy(self.mean_modules)
        for outcome_name in outcome_names:
            input_transform = self._get_input_transform(
                outcome_name, input_names, input_bounds, tkwargs
            )
            outcome_transform = Standardize(1)
            covar_module = self._get_module(covar_modules, outcome_name)
            mean_module = self.build_mean_module(
                outcome_name, mean_modules, input_transform, outcome_transform
            )

            # get training data
            train_X, train_Y, train_Yvar = get_training_data(
                input_names, outcome_name, data
            )
            # collect arguments into a single dict
            kwargs = {
                "input_transform": input_transform,
                "outcome_transform": outcome_transform,
                "covar_module": covar_module,
                "mean_module": mean_module,
            }

            if train_Yvar is None:
                # train basic single-task-gp model
                models.append(
                    self.build_single_task_gp(
                        train_X.to(**tkwargs),
                        train_Y.to(**tkwargs),
                        likelihood=self.get_likelihood(**tkwargs),
                        train=False,
                        **kwargs,
                    )
                )
            else:
                # train heteroskedastic single-task-gp model
                # turn off warnings
                models.append(
                    self.build_heteroskedastic_gp(
                        train_X.to(**tkwargs),
                        train_Y.to(**tkwargs),
                        train_Yvar.to(**tkwargs),
                        train=False,
                        **kwargs,
                    )
                )
        # check all specified modules were added to the model
        if covar_modules:
            warnings.warn(
                f"Covariance modules for output names {[k for k, v in self.covar_modules.items()]} "
                f"could not be added to the model."
            )
        if mean_modules:
            warnings.warn(
                f"Mean modules for output names {[k for k, v in self.mean_modules.items()]} "
                f"could not be added to the model."
            )

        full_model = ModelListGP(*models)

        # if specified, use cached model hyperparameters
        if self.use_cached_hyperparameters and self._hyperparameter_store is not None:
            store = {
                name: ele.to(**tkwargs)
                for name, ele in self._hyperparameter_store.items()
            }
            full_model.load_state_dict(store)

        if self.train_model:
            for m in full_model.models:
                mll = ExactMarginalLogLikelihood(m.likelihood, m)
                tr_kwargs = self.train_kwargs if self.train_kwargs is not None else {}
                fit_gpytorch_mll(mll, **tr_kwargs)

        # cache model hyperparameters
        self._hyperparameter_store = full_model.state_dict()

        return full_model.to(**tkwargs)

    def build_mean_module(
        self, name, mean_modules, input_transform, outcome_transform
    ) -> Optional[CustomMean]:
        """
        Build the mean module for the output specified by name.

        Parameters
        ----------
        name : str
            The name of the output.
        mean_modules: dict
            The dictionary of mean modules.
        input_transform : InputTransform
            Transform for input variables.
        outcome_transform : OutcomeTransform
            Transform for outcome variables.

        Returns
        -------
        Optional[CustomMean]
            The mean module for the output, or None if not specified.

        """
        mean_module = self._get_module(mean_modules, name)
        if mean_module is not None:
            fixed_model = False if name in self.trainable_mean_keys else True
            mean_module = CustomMean(
                mean_module, input_transform, outcome_transform, fixed_model=fixed_model
            )
        return mean_module

    @staticmethod
    def _get_module(base, name):
        """
        Get the module for a given name.

        Parameters
        ----------
        base : Union[Module, Dict[str, Module]]
            The base module or a dictionary of modules.
        name : str
            The name of the module.

        Returns
        -------
        Module
            The retrieved module.

        """
        if isinstance(base, Module):
            return deepcopy(base)
        elif isinstance(base, dict):
            return deepcopy(base.pop(name, None))
        else:
            return None

    def _get_input_transform(self, outcome_name, input_names, input_bounds, tkwargs):
        """get input transform based on the supplied bounds and attributes"""
        # get input bounds
        if input_bounds is None:
            bounds = None
        else:
            bounds = torch.vstack(
                [torch.tensor(input_bounds[name], **tkwargs) for name in input_names]
            ).T

        # create transform
        input_transform = Normalize(len(input_names), bounds=bounds)

        # remove input transform if the bool is False or the dict entry is false
        if isinstance(self.transform_inputs, bool):
            if not self.transform_inputs:
                input_transform = None
        if (
            isinstance(self.transform_inputs, dict)
            and outcome_name in self.transform_inputs
        ):
            if not self.transform_inputs[outcome_name]:
                input_transform = None

        # remove warnings if input transform is None
        if input_transform is None:
            botorch.settings.validate_input_scaling(False)

        return input_transform


class BatchedModelConstructor(StandardModelConstructor):
    def _get_input_transform(
        self,
        outcome_names: list[str],
        input_names: list[str],
        input_bounds,
        tkwargs,
        _aug_batch_shape: torch.Size = torch.Size([]),
    ) -> Optional[Normalize]:
        if input_bounds is None:
            bounds = None
        else:
            bounds = torch.vstack(
                [torch.tensor(input_bounds[name], **tkwargs) for name in input_names]
            ).T

        input_transform = Normalize(
            len(input_names),
            bounds=bounds,
            batch_shape=_aug_batch_shape,
        )

        # remove input transform if the bool is False or the dict entry is false
        if isinstance(self.transform_inputs, bool):
            if not self.transform_inputs:
                input_transform = None

        if isinstance(self.transform_inputs, dict):
            raise AttributeError(
                "Cannot specify dict for transform_inputs when using BatchedModelConstructor"
            )

        if input_transform is None:
            botorch.settings.validate_input_scaling(False)

        return input_transform

    def get_likelihood(
        self,
        dtype: torch.dtype = torch.double,
        device: Union[torch.device, str] = "cpu",
        _aug_batch_shape: torch.Size = torch.Size([]),
    ) -> Likelihood:
        tkwargs = {"dtype": dtype, "device": device}
        if self.custom_noise_prior is not None:
            likelihood = GaussianLikelihood(
                noise_prior=self.custom_noise_prior, batch_shape=_aug_batch_shape
            )
        elif self.use_low_noise_prior:
            likelihood = GaussianLikelihood(
                noise_prior=GammaPrior(1.0, 100.0), batch_shape=_aug_batch_shape
            )
        else:
            noise_prior = GammaPrior(1.1, 0.05)
            noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate
            likelihood = GaussianLikelihood(
                noise_prior=noise_prior,
                noise_constraint=GreaterThan(
                    MIN_INFERRED_NOISE_LEVEL,
                    transform=None,
                    initial_value=noise_prior_mode,
                ),
                batch_shape=_aug_batch_shape,
            )
        likelihood = likelihood.to(**tkwargs)
        return likelihood

    def get_training_data_batched(
        self, input_names: List[str], outcome_names: List[str], data: pd.DataFrame
    ) -> (torch.Tensor, torch.Tensor):
        """
        Returns
        -------
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]
            train_X (number of outcomes) x n x d
            train_Y (number of outcomes) x n x m
            train_Yvar (number of outcomes) x n x m

        """
        input_data = data[input_names]
        outcome_data = data[outcome_names]

        non_nans = ~input_data.isnull().T.any()
        non_nans_output = ~outcome_data.isnull().T.any()
        non_nans_all = non_nans & non_nans_output
        input_data = input_data[non_nans_all]
        outcome_data = outcome_data[non_nans_all]

        train_X = torch.tensor(input_data.to_numpy(dtype="double")).unsqueeze(0)
        # TODO: check expand vs repeat performance
        train_X = train_X.expand(len(outcome_names), -1, -1)

        train_Y_batches = []
        for outcome in outcome_names:
            train_Y = torch.tensor(
                outcome_data[outcome].to_numpy(dtype="double")
            ).unsqueeze(-1)
            train_Y_batches.append(train_Y)
        train_Y = torch.stack(train_Y_batches)

        return train_X, train_Y, None

    def build_model(
        self,
        input_names: List[str],
        outcome_names: List[str],
        data: pd.DataFrame,
        input_bounds: Dict[str, List] = None,
        dtype: torch.dtype = torch.double,
        device: Union[torch.device, str] = "cpu",
    ) -> SingleTaskGP:
        """
        Construct a single batched model for all objectives and constraints.
        """
        tkwargs = {"dtype": dtype, "device": device}

        if self.use_cached_hyperparameters:
            if self._hyperparameter_store is None:
                raise RuntimeWarning(
                    "cannot use cached hyperparameters, hyperparameter store empty, "
                    "training GP model hyperparameters instead"
                )

        train_X, train_Y, train_Yvar = self.get_training_data_batched(
            input_names, outcome_names, data
        )

        _num_outputs = train_Y.shape[-1]
        _input_batch_shape, _aug_batch_shape = (
            BatchedMultiOutputGPyTorchModel.get_batch_dimensions(
                train_X=train_X, train_Y=train_Y
            )
        )

        covar_modules = deepcopy(self.covar_modules)
        mean_modules = deepcopy(self.mean_modules)
        # mean_module = ConstantMean(batch_shape=self._aug_batch_shape)

        likelihood = self.get_likelihood(**tkwargs, _aug_batch_shape=_aug_batch_shape)
        input_transform = self._get_input_transform(
            outcome_names,
            input_names,
            input_bounds,
            tkwargs,
            _aug_batch_shape=_aug_batch_shape,
        )
        outcome_transform = Standardize(
            m=train_Y.shape[-1], batch_shape=_aug_batch_shape
        )
        # covar_module = self._get_module(covar_modules, outcome_name)
        # mean_module = self.build_mean_module(
        #     outcome_name, mean_modules, input_transform, outcome_transform
        # )
        kwargs = {
            "input_transform": input_transform,
            "outcome_transform": outcome_transform,
            # "covar_module": covar_module,
            # "mean_module": mean_module,
        }

        if train_X.shape[0] == 0 or train_Y.shape[0] == 0:
            raise ValueError("no data found to train model!")
        full_model = SingleTaskGP(train_X, train_Y, likelihood=likelihood, **kwargs)

        # check all specified modules were added to the model
        if covar_modules:
            warnings.warn(
                f"Covariance modules for output names {[k for k, v in self.covar_modules.items()]} "
                f"could not be added to the model."
            )
        if mean_modules:
            warnings.warn(
                f"Mean modules for output names {[k for k, v in self.mean_modules.items()]} "
                f"could not be added to the model."
            )

        # if specified, use cached model hyperparameters
        if self.use_cached_hyperparameters and self._hyperparameter_store is not None:
            store = {
                name: ele.to(**tkwargs)
                for name, ele in self._hyperparameter_store.items()
            }
            full_model.load_state_dict(store)

        if self.train_model:
            mll = ExactMarginalLogLikelihood(full_model.likelihood, full_model)
            tr_kwargs = self.train_kwargs if self.train_kwargs is not None else {}
            fit_gpytorch_mll(mll, **tr_kwargs)

        # cache model hyperparameters
        self._hyperparameter_store = full_model.state_dict()

        return full_model.to(**tkwargs)
