{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan roussel\\documents\\github\\xopt\\xopt\\bayesian\\optimize.py:254: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (ele.shape[-1] == len(vocs[vocs_type]),\n"
     ]
    }
   ],
   "source": [
    "# Import the class\n",
    "from xopt import Xopt\n",
    "from botorch.test_functions.multi_fidelity import AugmentedHartmann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Xopt` object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.\n",
    "\n",
    "Here we will make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a proper input file. \n",
    "YAML = \"\"\"\n",
    "xopt: {output_path: null, verbose: true}\n",
    "\n",
    "algorithm:\n",
    "  name: bayesian_optimization\n",
    "  options:  \n",
    "      n_initial_samples: 16\n",
    "      n_steps: 6\n",
    "      verbose: True\n",
    "      generator_options:\n",
    "          acquisition_function: custom_acq.acq\n",
    "          use_gpu: False\n",
    "\n",
    "simulation: \n",
    "  name: test_multi_fidelity\n",
    "  evaluate: xopt.evaluators.test_multi_fidelity.evaluate\n",
    "\n",
    "vocs:\n",
    "  name: test_multi_fidelity\n",
    "  description: null\n",
    "  simulation: test_multi_fidelity\n",
    "  templates: null\n",
    "  variables:\n",
    "    x1: [0, 1.0]\n",
    "    x2: [0, 1.0]\n",
    "    x3: [0, 1.0]\n",
    "    x4: [0, 1.0]\n",
    "    x5: [0, 1.0]\n",
    "    x6: [0, 1.0]\n",
    "    cost: [0, 1.0]                          ## NOTE: THIS IS REQUIRED FOR MULTI-FIDELITY OPTIMIZATION\n",
    "  objectives:\n",
    "    y1: 'MINIMIZE'\n",
    "  linked_variables: {}\n",
    "  constants: {a: dummy_constant}\n",
    "\n",
    "\"\"\"\n",
    "config = YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config as text\n",
      "Warning: No path set for key xopt : output_path\n",
      "Warning: No path set for key algorithm : options : restart_file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "            Xopt \n",
       "________________________________           \n",
       "Version: 0.4.3+98.gfc15b12.dirty\n",
       "Configured: True\n",
       "Config as YAML:\n",
       "xopt: {output_path: null, verbose: true, algorithm: cnsga}\n",
       "algorithm:\n",
       "  name: bayesian_optimization\n",
       "  function: xopt.bayesian.algorithms.bayesian_optimize\n",
       "  options:\n",
       "    n_initial_samples: 16\n",
       "    n_steps: 6\n",
       "    verbose: true\n",
       "    generator_options: {acquisition_function: !!python/name:custom_acq.acq '', use_gpu: false}\n",
       "    custom_model: null\n",
       "    restart_file: null\n",
       "    initial_x: null\n",
       "simulation:\n",
       "  name: test_multi_fidelity\n",
       "  evaluate: xopt.evaluators.test_multi_fidelity.evaluate\n",
       "  options: {extra_option: abc}\n",
       "vocs:\n",
       "  name: test_multi_fidelity\n",
       "  description: null\n",
       "  simulation: test_multi_fidelity\n",
       "  templates: null\n",
       "  variables:\n",
       "    x1: [0, 1.0]\n",
       "    x2: [0, 1.0]\n",
       "    x3: [0, 1.0]\n",
       "    x4: [0, 1.0]\n",
       "    x5: [0, 1.0]\n",
       "    x6: [0, 1.0]\n",
       "    cost: [0, 1.0]\n",
       "  objectives: {y1: MINIMIZE}\n",
       "  linked_variables: {}\n",
       "  constants: {a: dummy_constant}\n",
       "  constraints: {}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Xopt(config)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of these\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "#from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "executor = PoolExecutor()\n",
    "# This will also work. \n",
    "#executor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at time 2021-08-27T10:40:48-05:00\n",
      "started running optimization with generator: <xopt.bayesian.generators.generator.BayesianGenerator object at 0x000001D780B3AAF0>\n",
      "submitting initial candidates at time 2021-08-27T10:40:48-05:00\n",
      "starting optimization loop\n",
      "Model creation time: 0.096 s\n",
      "Candidate generation time: 0.44 s\n",
      "Candidate(s): tensor([[0.5986, 0.2744, 0.3984, 0.3156, 0.3382, 0.8192, 0.7050]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:49-05:00\n",
      "Model creation time: 0.116 s\n",
      "Candidate generation time: 0.393 s\n",
      "Candidate(s): tensor([[0.5780, 0.2772, 0.3839, 0.3045, 0.3146, 0.8329, 0.7266]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:49-05:00\n",
      "Model creation time: 0.147 s\n",
      "Candidate generation time: 0.418 s\n",
      "Candidate(s): tensor([[0.5655, 0.2793, 0.3753, 0.2990, 0.2990, 0.8413, 0.7394]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:50-05:00\n",
      "Model creation time: 0.175 s\n",
      "Candidate generation time: 0.377 s\n",
      "Candidate(s): tensor([[0.5654, 0.2769, 0.3750, 0.2927, 0.3054, 0.8401, 0.7389]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:50-05:00\n",
      "Model creation time: 0.157 s\n",
      "Candidate generation time: 0.323 s\n",
      "Candidate(s): tensor([[0.5552, 0.2670, 0.3667, 0.2583, 0.3233, 0.8411, 0.7457]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:51-05:00\n",
      "Model creation time: 0.2 s\n",
      "Candidate generation time: 0.448 s\n",
      "Candidate(s): tensor([[0.5515, 0.2668, 0.3639, 0.2503, 0.3247, 0.8379, 0.7445]],\n",
      "       dtype=torch.float64)\n",
      "submitting candidates at time 2021-08-27T10:40:51-05:00\n"
     ]
    }
   ],
   "source": [
    "# Change max generations\n",
    "X.run(executor=executor)\n",
    "results = X.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get highest fidelity global optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiFidelityGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RYANRO~1\\AppData\\Local\\Temp/ipykernel_17820/1680018698.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create generator object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiFidelityGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'MultiFidelityGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# create generator object\n",
    "gen = MultiFidelityGenerator(X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = create_multi_fidelity_model(results['variables'], results['corrected_objectives'], results['corrected_constraints'], X.vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: we want to get the minimum evaluated at the highest fidelity -> make sure to use get_recommendation\n",
    "rec = gen.get_recommendation(model)\n",
    "problem = AugmentedHartmann(negate=False)\n",
    "problem(rec) ## NOTE: the correct global minimum is -3.32237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
