{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Batched model tutorial\n",
    "In this tutorial we demonstrate that for problems where more than one output is involved (constraints and objectives)\n",
    "and\n",
    "you are ok using only perfect samples (no NaNs in any output), a significant speedup can be achieved by using a\n",
    "batched model. On GPU, this can get you 2-3x speedup."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:40.504418Z",
     "iopub.status.busy": "2024-09-13T15:54:40.503940Z",
     "iopub.status.idle": "2024-09-13T15:54:42.299260Z",
     "shell.execute_reply": "2024-09-13T15:54:42.298941Z"
    }
   },
   "source": "import numpy as np\nimport time\nfrom xopt.generators.bayesian.models.standard import BatchedModelConstructor, StandardModelConstructor\nfrom xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import ExpectedImprovementGenerator\nfrom xopt.numerical_optimizer import LBFGSOptimizer\nfrom xopt.resources.test_functions.rosenbrock import evaluate_rosenbrock\nfrom xopt import Xopt\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport threadpoolctl\n\ntorch.set_num_threads(1)\nthreadpoolctl.threadpool_limits(limits=1, user_api=\"blas\")\nthreadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n\nvocs = {\n    \"variables\": {f\"x{i}\": [-3,3] for i in range(16)},\n    \"objectives\": {\"y\": \"MINIMIZE\"},\n    \"constraints\": {\"c1\": [\"GREATER_THAN\", 0.1],\n                    \"c2\": [\"LESS_THAN\", 3],\n                    #\"c3\": [\"GREATER_THAN\", 0]\n                    },\n}\n\ndef eval_f(input_dict):\n    return {\"y\": np.sum(np.array([input_dict[f\"x{i}\"]**2 for i in range(16)])) + np.random.randn()*0.01,\n            #\"y\": evaluate_rosenbrock(input_dict)['y'],\n            \"y2\": input_dict[\"x0\"] + input_dict[\"x1\"],\n            \"c1\": input_dict[\"x2\"] + input_dict[\"x3\"],\n            \"c2\": input_dict[\"x4\"] + input_dict[\"x5\"]\n            }",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.301322Z",
     "iopub.status.busy": "2024-09-13T15:54:42.301113Z",
     "iopub.status.idle": "2024-09-13T15:54:42.303076Z",
     "shell.execute_reply": "2024-09-13T15:54:42.302830Z"
    }
   },
   "source": [
    "USE_CUDA = True\n",
    "evaluator = Evaluator(function=eval_f)\n",
    "generator = ExpectedImprovementGenerator(vocs=vocs,\n",
    "                                         gp_constructor=StandardModelConstructor(train_method='adam'),\n",
    "                                         numerical_optimizer=LBFGSOptimizer(n_restarts=5),\n",
    "                                         use_cuda=USE_CUDA)\n",
    "X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n",
    "generator_batched = ExpectedImprovementGenerator(vocs=vocs,\n",
    "                                                 gp_constructor=BatchedModelConstructor(train_method='adam'),\n",
    "                                                 numerical_optimizer=LBFGSOptimizer(n_restarts=5),\n",
    "                                                 use_cuda=USE_CUDA,\n",
    "                                                 )\n",
    "X2 = Xopt(evaluator=evaluator, generator=generator_batched, vocs=vocs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.304489Z",
     "iopub.status.busy": "2024-09-13T15:54:42.304389Z",
     "iopub.status.idle": "2024-09-13T15:54:42.309045Z",
     "shell.execute_reply": "2024-09-13T15:54:42.308828Z"
    }
   },
   "source": [
    "X.random_evaluate(20);\n",
    "X2.random_evaluate(20);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run the optimization\n",
    "We run the optimizers side by side to compare speed. In the interest of saving time, we skip 10 points by\n",
    " sampling randomly between optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.310443Z",
     "iopub.status.busy": "2024-09-13T15:54:42.310351Z",
     "iopub.status.idle": "2024-09-13T15:54:42.317339Z",
     "shell.execute_reply": "2024-09-13T15:54:42.317108Z"
    }
   },
   "source": [
    "history = []\n",
    "for i in range(50):\n",
    "    torch.cuda.empty_cache()\n",
    "    X.random_evaluate(10)\n",
    "\n",
    "    # sync data\n",
    "    X2.data = X.data.copy()\n",
    "    X2.generator.data = X.generator.data.copy()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    #X.generator.train_model()\n",
    "    X.step()\n",
    "    t2 = time.perf_counter()\n",
    "    X2.step()\n",
    "    #X2.generator.train_model()\n",
    "    t3 = time.perf_counter()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}\")\n",
    "    history.append({'n':len(X.data),\n",
    "                    'Standard training':X.generator.computation_time['training'].to_numpy()[-1],#t2-t1,\n",
    "                    'Standard acquisition':X.generator.computation_time['acquisition_optimization'].to_numpy()[-1],\n",
    "                    'Batched training':X2.generator.computation_time['training'].to_numpy()[-1],#t3-t2\n",
    "                    'Batched acquisition':X2.generator.computation_time['acquisition_optimization'].to_numpy()[-1],\n",
    "                    })"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plot performance\n",
    "Let's plot the timings."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(history_df['n'], history_df['Standard training'], label='Standard training')\n",
    "ax.plot(history_df['n'], history_df['Batched training'], label='Batched training')\n",
    "ax.set_ylabel('Time (s)')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.legend()\n",
    "ax.set_title(f'Vars: {len(vocs[\"variables\"])}, Objs: {len(vocs[\"objectives\"])}, Cons: {len(vocs[\"constraints\"])}, GPU: {generator.use_cuda}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(history_df['n'], history_df['Standard acquisition'], label='Standard acquisition')\n",
    "ax.plot(history_df['n'], history_df['Batched acquisition'], label='Batched acquisition')\n",
    "ax.set_ylabel('Time (s)')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}