{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Batched model tutorial\n",
    "In this tutorial we demonstrate that for problems where more than one output is involved (constraints and objectives)\n",
    "and\n",
    "you are ok using only perfect samples (no NaNs in any output), a significant speedup can be achieved by using a\n",
    "batched model. On GPU, this can get you 2-3x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:40.504418Z",
     "iopub.status.busy": "2024-09-13T15:54:40.503940Z",
     "iopub.status.idle": "2024-09-13T15:54:42.299260Z",
     "shell.execute_reply": "2024-09-13T15:54:42.298941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import threadpoolctl\n",
    "import torch\n",
    "\n",
    "from xopt import Xopt\n",
    "from xopt.evaluator import Evaluator\n",
    "from xopt.generators.bayesian import ExpectedImprovementGenerator\n",
    "from xopt.generators.bayesian.models.standard import (\n",
    "    BatchedModelConstructor,\n",
    "    StandardModelConstructor,\n",
    ")\n",
    "from xopt.numerical_optimizer import LBFGSOptimizer\n",
    "from xopt.resources.test_functions.rosenbrock import evaluate_rosenbrock\n",
    "\n",
    "HAS_CUDA = False\n",
    "step_size = 2\n",
    "if torch.cuda.is_available():\n",
    "    step_size = 20\n",
    "    HAS_CUDA = True\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"blas\")\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "\n",
    "vocs = {\n",
    "    \"variables\": {f\"x{i}\": [-3, 3] for i in range(16)},\n",
    "    \"objectives\": {\"y\": \"MINIMIZE\"},\n",
    "    \"constraints\": {\n",
    "        \"c1\": [\"GREATER_THAN\", 0.1],\n",
    "        \"c2\": [\"LESS_THAN\", 3],\n",
    "        \"c3\": [\"GREATER_THAN\", 0],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def eval_f(input_dict):\n",
    "    rosenbrock = False  # change to try another function\n",
    "    d = {\n",
    "        \"y2\": input_dict[\"x0\"] + input_dict[\"x1\"],\n",
    "        \"c1\": input_dict[\"x2\"] + input_dict[\"x3\"],\n",
    "        \"c2\": input_dict[\"x4\"] + input_dict[\"x5\"],\n",
    "        \"c3\": input_dict[\"x0\"],\n",
    "    }\n",
    "    if rosenbrock:\n",
    "        d[\"y\"] = evaluate_rosenbrock(input_dict)[\"y\"]\n",
    "    else:\n",
    "        d[\"y\"] = (\n",
    "            np.sum(np.array([input_dict[f\"x{i}\"] ** 2 for i in range(16)]))\n",
    "            + np.random.randn() * 0.01\n",
    "        )\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.301322Z",
     "iopub.status.busy": "2024-09-13T15:54:42.301113Z",
     "iopub.status.idle": "2024-09-13T15:54:42.303076Z",
     "shell.execute_reply": "2024-09-13T15:54:42.302830Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(function=eval_f)\n",
    "generator = ExpectedImprovementGenerator(\n",
    "    vocs=vocs,\n",
    "    gp_constructor=StandardModelConstructor(train_method=\"adam\"),\n",
    "    numerical_optimizer=LBFGSOptimizer(n_restarts=5),\n",
    "    use_cuda=HAS_CUDA,\n",
    ")\n",
    "X = Xopt(evaluator=evaluator, generator=generator)\n",
    "generator_batched = ExpectedImprovementGenerator(\n",
    "    vocs=vocs,\n",
    "    gp_constructor=BatchedModelConstructor(train_method=\"adam\"),\n",
    "    numerical_optimizer=LBFGSOptimizer(n_restarts=5),\n",
    "    use_cuda=HAS_CUDA,\n",
    ")\n",
    "X2 = Xopt(evaluator=evaluator, generator=generator_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.304489Z",
     "iopub.status.busy": "2024-09-13T15:54:42.304389Z",
     "iopub.status.idle": "2024-09-13T15:54:42.309045Z",
     "shell.execute_reply": "2024-09-13T15:54:42.308828Z"
    }
   },
   "outputs": [],
   "source": [
    "X.random_evaluate(20)\n",
    "X2.random_evaluate(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run the optimization\n",
    "We run the optimizers side by side to compare speed. In the interest of saving time, we skip 10 points by\n",
    " sampling randomly between optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:54:42.310443Z",
     "iopub.status.busy": "2024-09-13T15:54:42.310351Z",
     "iopub.status.idle": "2024-09-13T15:54:42.317339Z",
     "shell.execute_reply": "2024-09-13T15:54:42.317108Z"
    }
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "for i in range(50):\n",
    "    torch.cuda.empty_cache()\n",
    "    X.random_evaluate(step_size)\n",
    "\n",
    "    # sync data\n",
    "    X2.data = X.data.copy()\n",
    "    X2.generator.data = X.generator.data.copy()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    X.step()\n",
    "    t2 = time.perf_counter()\n",
    "    X2.step()\n",
    "    t3 = time.perf_counter()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}\")\n",
    "    history.append(\n",
    "        {\n",
    "            \"n\": len(X.data),\n",
    "            \"Standard training\": X.generator.computation_time[\"training\"].to_numpy()[\n",
    "                -1\n",
    "            ],  # t2-t1,\n",
    "            \"Standard acquisition\": X.generator.computation_time[\n",
    "                \"acquisition_optimization\"\n",
    "            ].to_numpy()[-1],\n",
    "            \"Batched training\": X2.generator.computation_time[\"training\"].to_numpy()[\n",
    "                -1\n",
    "            ],  # t3-t2\n",
    "            \"Batched acquisition\": X2.generator.computation_time[\n",
    "                \"acquisition_optimization\"\n",
    "            ].to_numpy()[-1],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance\n",
    "Let's plot the timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(history_df[\"n\"], history_df[\"Standard training\"], label=\"Standard training\")\n",
    "ax.plot(history_df[\"n\"], history_df[\"Batched training\"], label=\"Batched training\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.legend()\n",
    "ax.set_title(\n",
    "    f\"Vars: {len(vocs['variables'])}, Objs: {len(vocs['objectives'])}, Cons: {len(vocs['constraints'])}, GPU: {generator.use_cuda}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(\n",
    "    history_df[\"n\"],\n",
    "    history_df[\"Standard acquisition\"],\n",
    "    label=\"Standard acquisition\",\n",
    ")\n",
    "ax.plot(history_df[\"n\"], history_df[\"Batched acquisition\"], label=\"Batched acquisition\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
