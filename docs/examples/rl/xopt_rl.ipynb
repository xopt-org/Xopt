{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class XoptEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A simple custom environment for xopt-style optimization using RL.\n",
    "    The agent attempts to maximize the value of the function: f(x) = -(x-2)^2 + 4\n",
    "    The optimal value is 4 at x=2.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, target_function, initial_x_range=(-5, 5), action_scale=0.1):\n",
    "        super(XoptEnv, self).__init__()\n",
    "\n",
    "        self.variables = {\"x\": [-5.0, 5.0]}  # xopt's variable boundaries\n",
    "\n",
    "        self.target_function = target_function\n",
    "\n",
    "        low = np.array([initial_x_range[0]], dtype=np.float32)\n",
    "        high = np.array([initial_x_range[1]], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low, high=high, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_scale = action_scale\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0], dtype=np.float32),\n",
    "            high=np.array([1.0], dtype=np.float32),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.current_x = None\n",
    "        self.initial_x_range = initial_x_range\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Helper to get the observation\"\"\"\n",
    "        return np.array([self.current_x], dtype=np.float32)\n",
    "\n",
    "    def _calculate_reward(self, x):\n",
    "        \"\"\"The reward is the objective value we want to maximize.\"\"\"\n",
    "        return self.target_function(x)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Initializes the environment, often by sampling a starting point for 'x'.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Sample a random starting x within the initial range\n",
    "        self.current_x = self.np_random.uniform(\n",
    "            low=self.initial_x_range[0], high=self.initial_x_range[1]\n",
    "        )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applies the action (delta_x) to the current state, evaluates the function,\n",
    "        and returns the reward.\n",
    "        \"\"\"\n",
    "        delta_x = action[0] * self.action_scale\n",
    "\n",
    "        new_x = self.current_x + delta_x\n",
    "\n",
    "        x_min, x_max = self.variables[\"x\"]\n",
    "        self.current_x = np.clip(new_x, x_min, x_max)\n",
    "\n",
    "        reward = self._calculate_reward(self.current_x)\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {\"objective_value\": reward}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "N_ENVS = 4\n",
    "\n",
    "\n",
    "def target_function(x):\n",
    "    \"\"\"Calculates the value of -(x - 2.0)**2 + 4.0.\"\"\"\n",
    "    return -((x - 2.0) ** 2) + 4.0\n",
    "\n",
    "\n",
    "vec_env = XoptEnv(target_function)\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    gamma=0.99,\n",
    "    n_steps=256,\n",
    "    ent_coef=0.01,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "TIMESTEPS = 25000\n",
    "print(f\"Starting PPO training for {TIMESTEPS} timesteps...\")\n",
    "model.learn(total_timesteps=TIMESTEPS)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xopt.generators.rl import RLModelGenerator\n",
    "from xopt import Xopt, Evaluator\n",
    "\n",
    "\n",
    "def objective_function(input_data: dict) -> dict:\n",
    "    x = input_data[\"x\"]\n",
    "    return {\"f\": target_function(x)}\n",
    "\n",
    "\n",
    "# TODO This is currently unused. Needs to align with the env, the objective function, and the xopt model wrapper\n",
    "vocs = {\"variables\": {\"x\": [-5.0, 5.0]}, \"objectives\": {\"f\": \"MAXIMIZE\"}}\n",
    "\n",
    "\n",
    "generator = RLModelGenerator(vocs=vocs)\n",
    "generator.set_model(model)\n",
    "\n",
    "evaluator = Evaluator(function=objective_function)\n",
    "\n",
    "X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\n",
    "X.max_evaluations = 100\n",
    "\n",
    "\n",
    "print(\"\\n--- Xopt Configuration Summary ---\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nPerforming 2 random initial evaluations...\")\n",
    "X.random_evaluate(2)\n",
    "\n",
    "print(\"\\n--- Initial Optimization Results ---\")\n",
    "print(X.data)\n",
    "\n",
    "X.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopt-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
