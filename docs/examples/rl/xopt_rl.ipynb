{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class XoptEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom environment for xopt-style optimization using RL.\n",
    "    The agent attempts to maximize a target function of N variables.\n",
    "    The observation space is the current value of the variables (x1, x2, ...).\n",
    "    The action space is the step/delta to apply to each variable (dx1, dx2, ...).\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, target_function, variables, action_scale=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the environment.\n",
    "\n",
    "        :param target_function: The callable function to maximize, which accepts\n",
    "                                a NumPy array of variable values.\n",
    "        :param variables: A dictionary mapping variable names to their [min, max] bounds.\n",
    "                          Example: {\"x1\": [-5.0, 5.0], \"x2\": [0.0, 10.0]}\n",
    "        :param action_scale: The factor by which the normalized action [-1, 1] is scaled.\n",
    "        \"\"\"\n",
    "        super(XoptEnv, self).__init__()\n",
    "\n",
    "        self.variables = OrderedDict(variables)\n",
    "        self.var_names = list(self.variables.keys())\n",
    "        self.n_variables = len(self.variables)\n",
    "\n",
    "        self.target_function = target_function\n",
    "        self.action_scale = action_scale\n",
    "\n",
    "        var_bounds = np.array(list(self.variables.values()), dtype=np.float32)\n",
    "\n",
    "        low_obs = var_bounds[:, 0]\n",
    "        high_obs = var_bounds[:, 1]\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low_obs, high=high_obs, shape=(self.n_variables,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # --- Define Action Space (Deltas for each Variable) ---\n",
    "        # The action is a vector of N dimensions, each in [-1.0, 1.0]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0] * self.n_variables, dtype=np.float32),\n",
    "            high=np.array([1.0] * self.n_variables, dtype=np.float32),\n",
    "            shape=(self.n_variables,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # current_x will now be a NumPy array of shape (N,)\n",
    "        self.current_x = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Helper to get the observation (current variable values)\"\"\"\n",
    "        return self.current_x.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, x_array):\n",
    "        \"\"\"\n",
    "        The reward is the objective value we want to maximize.\n",
    "        x_array is a NumPy array of all variable values.\n",
    "        \"\"\"\n",
    "        # The target function must be able to handle an N-dimensional array input\n",
    "        return self.target_function(x_array)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Initializes the environment by sampling a starting point for all variables.\n",
    "        The starting point is sampled uniformly between the bounds for each variable.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Get the variable bounds for uniform sampling\n",
    "        var_bounds = np.array(list(self.variables.values()), dtype=np.float32)\n",
    "        low_bounds = var_bounds[:, 0]\n",
    "        high_bounds = var_bounds[:, 1]\n",
    "\n",
    "        # Sample a random starting point for ALL variables\n",
    "        self.current_x = self.np_random.uniform(\n",
    "            low=low_bounds, high=high_bounds, size=self.n_variables\n",
    "        )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Applies the action (delta_x vector) to the current state, evaluates the function,\n",
    "        and returns the reward.\n",
    "\n",
    "        :param action: A NumPy array of shape (N,) from the action space,\n",
    "                       where each element is in [-1.0, 1.0].\n",
    "        \"\"\"\n",
    "        # Scale the action vector: delta_x = action * action_scale\n",
    "        delta_x = action * self.action_scale\n",
    "\n",
    "        # Calculate the new, unconstrained variable values\n",
    "        new_x = self.current_x + delta_x\n",
    "\n",
    "        # Get the min/max bounds for all variables\n",
    "        var_bounds = np.array(list(self.variables.values()), dtype=np.float32)\n",
    "        x_min = var_bounds[:, 0]\n",
    "        x_max = var_bounds[:, 1]\n",
    "\n",
    "        # Apply bounds clipping to ALL variables\n",
    "        self.current_x = np.clip(new_x, x_min, x_max)\n",
    "\n",
    "        # Calculate the reward using the N-dimensional state\n",
    "        reward = self._calculate_reward(self.current_x)\n",
    "\n",
    "        terminated = False  # RL usually runs until a termination condition is met (e.g., max steps)\n",
    "        truncated = False\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {\"objective_value\": reward}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "N_ENVS = 4\n",
    "\n",
    "\n",
    "def quadratic_n_dim(x):\n",
    "    \"\"\"Target function: f(x) = -sum((xi - 2)^2) + 4 * N_dim\"\"\"\n",
    "    # The optimal value is 4 * N_dim when all xi = 2\n",
    "    n_dim = len(x)\n",
    "    return -np.sum((x - 2.0) ** 2) + 4.0 * n_dim\n",
    "\n",
    "\n",
    "# Define variables for a 3D problem\n",
    "three_d_variables = {\"x1\": [-5.0, 5.0], \"x2\": [-10.0, 10.0], \"x3\": [0.0, 5.0]}\n",
    "\n",
    "# Create the environment\n",
    "env = XoptEnv(\n",
    "    target_function=quadratic_n_dim, variables=three_d_variables, action_scale=0.5\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    gamma=0.99,\n",
    "    n_steps=256,\n",
    "    ent_coef=0.01,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "TIMESTEPS = 25000\n",
    "print(f\"Starting PPO training for {TIMESTEPS} timesteps...\")\n",
    "model.learn(total_timesteps=TIMESTEPS)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xopt.generators.rl import RLModelGenerator\n",
    "from xopt import Xopt, Evaluator\n",
    "from xopt.vocs import VOCS\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "def quadratic_n_dim(x):\n",
    "    \"\"\"\n",
    "    Target function: f(x) = -sum((x_i - 2)^2) + 4 * N_dim\n",
    "    Accepts a single NumPy array 'x' of shape (N,)\n",
    "    \"\"\"\n",
    "    n_dim = len(x)\n",
    "    return -np.sum((x - 2.0) ** 2) + 4.0 * n_dim\n",
    "\n",
    "\n",
    "def objective_function(input_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Wraps the target_function for Xopt's Evaluator.\n",
    "    It expects a dictionary where keys are variable names (e.g., 'x1', 'x2').\n",
    "    \"\"\"\n",
    "    x_array = np.array([input_data[\"x1\"], input_data[\"x2\"], input_data[\"x3\"]])\n",
    "\n",
    "    return {\"f\": quadratic_n_dim(x_array)}\n",
    "\n",
    "\n",
    "multi_variable_vocs_data = {\n",
    "    \"variables\": {\"x1\": [-5.0, 5.0], \"x2\": [-10.0, 10.0], \"x3\": [0.0, 5.0]},\n",
    "    \"objectives\": {\"f\": \"MAXIMIZE\"},\n",
    "}\n",
    "vocs = VOCS(**multi_variable_vocs_data)\n",
    "\n",
    "generator = RLModelGenerator(vocs=vocs)\n",
    "generator.set_model(model)\n",
    "\n",
    "evaluator = Evaluator(function=objective_function)\n",
    "\n",
    "X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\n",
    "X.max_evaluations = 10\n",
    "\n",
    "\n",
    "print(\"\\n--- Xopt Configuration Summary ---\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nPerforming 2 random initial evaluations...\")\n",
    "X.random_evaluate(2)\n",
    "\n",
    "print(\"\\n--- Initial Optimization Results ---\")\n",
    "print(X.data[[\"x1\", \"x2\", \"x3\", \"f\"]])\n",
    "\n",
    "print(f\"\\nRunning Xopt for {X.max_evaluations - len(X.data)} more iterations...\")\n",
    "X.run()\n",
    "\n",
    "print(\"\\n--- Final Optimization Results ---\")\n",
    "print(X.data[[\"x1\", \"x2\", \"x3\", \"f\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopt-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
